# XPER¬†Clustering

> **Graduate Thesis Repository ‚Äì _Mitigating Performance‚ÄØHeterogeneity via XPER‚ÄëBased Clustering_**

---

**X**‚Äëplainable **Per**formance (XPER) is a novel model‚Äëagnostic interpretability framework that measures the marginal contribution of a particular feature to the predictive performance of a regression or classification model ([Hu√© et al., 2022](https://www.researchgate.net/publication/366212631_Explainable_Performance)). 

This repository provides all code to reproduce the experiments and figures for my graduate thesis, in which I investigate whether **clustering instances on their local XPER vectors can reduce model‚Äëperformance heterogeneity**, and secondly whether building cluster specific models improves model generalizability.

<p align="center">
  <em>Methodology overview</em><br>
  <!-- Update the path once the image is committed -->
  <img src="assets/image.png" width="650" alt="Flowchart of the experimental pipeline"/>
</p>

---

## Key¬†Idea

1. **Train a baseline XGBoost model** on the *credit_risk* dataset.
2. **Compute local XPER vectors** for every training instance.
3. **Cluster** these vectors with *K‚ÄëMeans*, *K‚ÄëMedoids*, and *Gaussian Mixture Models* (GMM).
4. **Retrain cluster‚Äëspecific XGBoost models** on the raw features of each cluster.
5. **Evaluate** on test data that are first assigned to a cluster via the pretrained clustering model.
6. **Benchmark** against models trained on clusters built from the original (untransformed) feature space.
7. **Analyse and visualise** performance, heterogeneity, and feature contributions.

All artefacts (data splits, models, plots, metrics) are stored in a timestamped directory under `experiments/`.

---

## Repository¬†Structure

```text
XPER_clustering/
‚îú‚îÄ‚îÄ analysis/               # Notebooks & scripts for extended analysis
‚îÇ   ‚îú‚îÄ‚îÄ aggregate_experiment_analysis.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ baseline_model_analysis.py
‚îÇ   ‚îú‚îÄ‚îÄ cluster_analysis.py
‚îÇ   ‚îú‚îÄ‚îÄ data_split_consistency.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ exploratory_analysis.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ feature_versus_xper_outliers.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ heterogeneity.ipynb
‚îú‚îÄ‚îÄ core/                   # Main experiment pipelines
‚îÇ   ‚îú‚îÄ‚îÄ cluster_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ global_pipeline.py
‚îú‚îÄ‚îÄ data/                   # Local copies of public datasets & loader
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ credit_risk/
‚îÇ   ‚îî‚îÄ‚îÄ loader.py
‚îú‚îÄ‚îÄ experiments/            # Auto‚Äëgenerated experiment outputs (git‚Äëignored)
‚îú‚îÄ‚îÄ utils/                  # Helper functions
‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îú‚îÄ‚îÄ visualization/          # Plotting utilities
‚îÇ   ‚îú‚îÄ‚îÄ cluster_visualizations.py
‚îÇ   ‚îî‚îÄ‚îÄ global_visualizations.py
‚îú‚îÄ‚îÄ config.py               # Central experiment configuration
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md               # You are here üöÄ
```

---

## ‚öôÔ∏è¬†Requirements

* **Python¬†>=¬†3.12**
* All packages listed in `requirements.txt`

### Quick¬†Install

```bash
# (Optional) create an isolated environment
python3.12 -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

## Getting¬†Started

1. **Clone the repo**
   ```bash
   git clone https://github.com/schronic/xper_clustering.git
   cd xper-clustering
   ```
2. **Configure your run** ‚Äì open `config.py` and adjust parameters such as:
   | Variable | Purpose |
   |----------|---------|
   | `SAMPLE_SIZE` | Number of rows sampled from the dataset |
   | `N_FEATURES`  | Number of features retained |
   | `CLUSTERS`    | Fixed \(k\) or `None` to use Silhouette‚Äëbased optimisation |
   | `KERNEL_USE`  | Enable RBF kernel approximation (bool) |
3. **Run the global pipeline**
   ```bash
   python -m core.global_pipeline
   ```
   ‚û°Ô∏è Creates `experiments/experiment_YYYYMMDD_HHMMSS/` with base models, XPER files, and first plots.
4. **Deep‚Äëdive per‚Äëcluster**
   *Edit the `EXPERIMENT_FOLDER` at the top of `core/cluster_pipeline.py` to point to the folder created in step¬†3, then run:*
   ```bash
   python -m core.cluster_pipeline
   ```
5. **Aggregate across experiments** (optional)
   ```bash
   jupyter notebook analysis/aggregate_experiment_analysis.ipynb
   # or run the analysis script directly
   python analysis/cluster_analysis.py
   ```

All thesis‚Äërelated experiment outputs are also available in [this shared Google¬†Drive folder](https://drive.google.com/drive/folders/1kPobSBmtKVLTsMx6glIjV_CHd4ns2Z1F?usp=sharing).

---

## Outputs & Logs

Each experiment folder contains:

| Path | Contents |
|------|----------|
| `models/` | Global & cluster‚Äëspecific XGBoost binaries |
| `data/`   | Train/Test CSVs, cluster assignments |
| `xper_values/` | Local & global XPER CSVs |
| `visualizations/` | PNG/HTML plots generated during the run |
| `final_results.csv` | Metrics for the given dataset |
| `overall_results.csv` | Aggregated metrics across datasets |

Additional files produced by `analysis/` scripts include combined Excel sheets (`*_combined_analysis_frames.xlsx`) and inter‚Äëexperiment comparison plots housed in `analysis_plots_clusters/`.

---